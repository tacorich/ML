{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Copy of kaggle-cifar10.ipynb","provenance":[{"file_id":"https://github.com/d2l-ai/d2l-en-colab/blob/master/chapter_computer-vision/kaggle-cifar10.ipynb","timestamp":1606146514715}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"J8MrOic-w0pB"},"source":["The following additional libraries are needed to run this\n","notebook. Note that running on Colab is experimental, please report a Github\n","issue if you have any problem."]},{"cell_type":"code","metadata":{"id":"UnUiQpVPw0pB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606154473200,"user_tz":300,"elapsed":85305,"user":{"displayName":"taco rich","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp4OrQNUcok6g2RgLONXM6CjW0d0tw7tSqyd7G=s64","userId":"12470220693402591167"}},"outputId":"391b74e7-858d-4177-c2f7-3c76230d6f4d"},"source":["!pip install d2l==0.15.1\n","!pip install -U mxnet-cu101==1.7.0\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting d2l==0.15.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/fd/89b6b8fd34b4e2e54fadf5de6e8f63fd96e0c14d2b6c81ba40e9edcd964a/d2l-0.15.1-py3-none-any.whl (61kB)\n","\r\u001b[K     |█████▍                          | 10kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 20kB 15.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 30kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 40kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 51kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.1MB/s \n","\u001b[?25hRequirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from d2l==0.15.1) (1.0.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from d2l==0.15.1) (1.1.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from d2l==0.15.1) (1.18.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from d2l==0.15.1) (3.2.2)\n","Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.15.1) (4.7.7)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.15.1) (5.6.1)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.15.1) (4.10.1)\n","Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.15.1) (5.2.0)\n","Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.15.1) (7.5.1)\n","Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.15.1) (5.3.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->d2l==0.15.1) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->d2l==0.15.1) (2.8.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.15.1) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.15.1) (1.3.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.15.1) (2.4.7)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.15.1) (0.2.0)\n","Requirement already satisfied: jupyter-client>=4.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.15.1) (5.3.5)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.15.1) (2.6.1)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.15.1) (4.7.0)\n","Requirement already satisfied: traitlets in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.15.1) (4.3.3)\n","Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.15.1) (1.9.0)\n","Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.15.1) (20.0.0)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.1) (0.4.4)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.1) (0.3)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.1) (1.4.3)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.1) (0.8.4)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.1) (0.6.0)\n","Requirement already satisfied: nbformat>=4.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.1) (5.0.8)\n","Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.1) (2.11.2)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.1) (3.2.1)\n","Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->d2l==0.15.1) (5.5.0)\n","Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->d2l==0.15.1) (5.1.1)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->d2l==0.15.1) (1.0.18)\n","Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->d2l==0.15.1) (3.5.1)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.15.1) (0.9.1)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.15.1) (1.5.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->d2l==0.15.1) (1.15.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets->qtconsole->jupyter->d2l==0.15.1) (4.4.2)\n","Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert->jupyter->d2l==0.15.1) (2.6.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert->jupyter->d2l==0.15.1) (1.1.1)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->d2l==0.15.1) (0.5.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->d2l==0.15.1) (20.4)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.15.1) (0.7.5)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.15.1) (50.3.2)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.15.1) (0.8.1)\n","Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.15.1) (4.8.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->d2l==0.15.1) (0.2.5)\n","Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.8.1->notebook->jupyter->d2l==0.15.1) (0.6.0)\n","Installing collected packages: d2l\n","Successfully installed d2l-0.15.1\n","Collecting mxnet-cu101==1.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/26/9655677b901537f367c3c473376e4106abc72e01a8fc25b1cb6ed9c37e8c/mxnet_cu101-1.7.0-py2.py3-none-manylinux2014_x86_64.whl (846.0MB)\n","\u001b[K     |███████████████████████████████▌| 834.1MB 1.2MB/s eta 0:00:10tcmalloc: large alloc 1147494400 bytes == 0x64774000 @  0x7f420ec70615 0x591e47 0x4cc179 0x4cc2db 0x50a1cc 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50cc96 0x58e683 0x50c127 0x58e683 0x50c127 0x58e683 0x50c127 0x58e683 0x50c127 0x5095c8 0x50a2fd 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n","\u001b[K     |████████████████████████████████| 846.0MB 19kB/s \n","\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1\n","  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101==1.7.0) (1.18.5)\n","Requirement already satisfied, skipping upgrade: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101==1.7.0) (2.23.0)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101==1.7.0) (2.10)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101==1.7.0) (3.0.4)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101==1.7.0) (2020.11.8)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101==1.7.0) (1.24.3)\n","Installing collected packages: graphviz, mxnet-cu101\n","  Found existing installation: graphviz 0.10.1\n","    Uninstalling graphviz-0.10.1:\n","      Successfully uninstalled graphviz-0.10.1\n","Successfully installed graphviz-0.8.4 mxnet-cu101-1.7.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fa2ngm14w0pB"},"source":["# Image Classification (CIFAR-10) on Kaggle\n",":label:`sec_kaggle_cifar10`\n","\n","So far, we have been using Gluon's `data` package to directly obtain image datasets in the tensor format. In practice, however, image datasets often exist in the format of image files. In this section, we will start with the original image files and organize, read, and convert the files to the tensor format step by step.\n","\n","We performed an experiment on the CIFAR-10 dataset in :numref:`sec_image_augmentation`.\n","This is an important data\n","set in the computer vision field. Now, we will apply the knowledge we learned in\n","the previous sections in order to participate in the Kaggle competition, which\n","addresses CIFAR-10 image classification problems. The competition's web address\n","is\n","\n","> https://www.kaggle.com/c/cifar-10\n","\n",":numref:`fig_kaggle_cifar10` shows the information on the competition's webpage. In order to submit the results, please register an account on the Kaggle website first.\n","\n","![CIFAR-10 image classification competition webpage information. The dataset for the competition can be accessed by clicking the \"Data\" tab.](https://github.com/d2l-ai/d2l-en-colab/blob/master/img/kaggle-cifar10.png?raw=1)\n",":width:`600px`\n",":label:`fig_kaggle_cifar10`\n","\n","First, import the packages or modules required for the competition.\n"]},{"cell_type":"code","metadata":{"id":"TK7NKWk8w0pB","executionInfo":{"status":"ok","timestamp":1606160488482,"user_tz":300,"elapsed":365,"user":{"displayName":"taco rich","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp4OrQNUcok6g2RgLONXM6CjW0d0tw7tSqyd7G=s64","userId":"12470220693402591167"}}},"source":["import collections\n","from d2l import mxnet as d2l\n","import math\n","from mxnet import autograd, gluon, init, npx\n","from mxnet.gluon import nn\n","import os\n","import pandas as pd\n","import shutil\n","import time\n","import zipfile\n","\n","npx.set_np()"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"74jg-0lEw0pB"},"source":["## Obtaining and Organizing the Dataset\n","\n","The competition data is divided into a training set and testing set. The training set contains $50,000$ images. The testing set contains $300,000$ images, of which $10,000$ images are used for scoring, while the other $290,000$ non-scoring images are included to prevent the manual labeling of the testing set and the submission of labeling results. The image formats in both datasets are PNG, with heights and widths of 32 pixels and three color channels (RGB). The images cover $10$ categories: planes, cars, birds, cats, deer, dogs, frogs, horses, boats, and trucks. The upper-left corner of :numref:`fig_kaggle_cifar10` shows some images of planes, cars, and birds in the dataset.\n","\n","### Downloading the Dataset\n","\n","After logging in to Kaggle, we can click on the \"Data\" tab on the CIFAR-10 image classification competition webpage shown in :numref:`fig_kaggle_cifar10` and download the dataset by clicking the \"Download All\" button. After unzipping the downloaded file in `../data`, and unzipping `train.7z` and `test.7z` inside it, you will find the entire dataset in the following paths:\n","\n","* ../data/cifar-10/train/[1-50000].png\n","* ../data/cifar-10/test/[1-300000].png\n","* ../data/cifar-10/trainLabels.csv\n","* ../data/cifar-10/sampleSubmission.csv\n","\n","Here folders `train` and `test` contain the training and testing images respectively, `trainLabels.csv` has labels for the training images, and `sample_submission.csv` is a sample of submission.\n","\n","To make it easier to get started, we provide a small-scale sample of the dataset: it contains the first $1000$ training images and $5$ random testing images.\n","To use the full dataset of the Kaggle competition, you need to set the following `demo` variable to `False`.\n"]},{"cell_type":"code","metadata":{"id":"NWWbzmVZw0pB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606162047616,"user_tz":300,"elapsed":49989,"user":{"displayName":"taco rich","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp4OrQNUcok6g2RgLONXM6CjW0d0tw7tSqyd7G=s64","userId":"12470220693402591167"}},"outputId":"83e41139-e4ff-4bb1-a485-47c6dfde6188"},"source":["#@save\n","d2l.DATA_HUB['cifar10_tiny'] = (d2l.DATA_URL + 'kaggle_cifar10_tiny.zip',\n","                                '2068874e4b9a9f0fb07ebe0ad2b29754449ccacd')\n","\n","# If you use the full dataset downloaded for the Kaggle competition, set\n","# `demo` to False\n","demo = False\n","from google.colab import drive\n","\n","drive.mount(\"/content/drive/\", force_remount=True)\n","\n","drive.mount('/content/drive/')\n","\n","!unzip -q \"/content/drive/MyDrive/ML/Dog/dog-breed-identification.zip\"  -d \"/content/Dog'\n","data_dir = (\"/content/drive/MyDrive/ML/cifar-10\")\n","\n","\n","\n"],"execution_count":48,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n","Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4OQ2lOR5w0pB"},"source":["### Organizing the Dataset\n","\n","We need to organize datasets to facilitate model training and testing. Let us first read the labels from the csv file. The following function returns a dictionary that maps the filename without extension to its label.\n"]},{"cell_type":"code","metadata":{"id":"rDdnhNLew0pB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606162077091,"user_tz":300,"elapsed":317,"user":{"displayName":"taco rich","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp4OrQNUcok6g2RgLONXM6CjW0d0tw7tSqyd7G=s64","userId":"12470220693402591167"}},"outputId":"a7c9a150-e9ff-4097-e844-a830ec4c564a"},"source":["#@save\n","def read_csv_labels(fname):\n","    \"\"\"Read fname to return a name to label dictionary.\"\"\"\n","    with open(fname, 'r') as f:\n","        # Skip the file header line (column name)\n","        lines = f.readlines()[1:]\n","    tokens = [l.rstrip().split(',') for l in lines]\n","    return dict(((name, label) for name, label in tokens))\n","\n","labels = read_csv_labels(os.path.join(data_dir, 'trainLabels.csv'))\n","print('# training examples:', len(labels))\n","print('# classes:', len(set(labels.values())))"],"execution_count":49,"outputs":[{"output_type":"stream","text":["# training examples: 50000\n","# classes: 10\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eGrHC-X1w0pC"},"source":["Next, we define the `reorg_train_valid` function to segment the validation set from the original training set. The argument `valid_ratio` in this function is the ratio of the number of examples in the validation set to the number of examples in the original training set. In particular, let $n$ be the number of images of the class with the least examples, and $r$ be the ratio, then we will use $\\max(\\lfloor nr\\rfloor,1)$ images for each class as the validation set.  Let us use `valid_ratio=0.1` as an example. Since the original training set has $50,000$ images, there will be $45,000$ images used for training and stored in the path \"`train_valid_test/train`\" when tuning hyperparameters, while the other $5,000$ images will be stored as validation set in the path \"`train_valid_test/valid`\". After organizing the data, images of the same class will be placed under the same folder so that we can read them later.\n"]},{"cell_type":"code","metadata":{"id":"vZNH5OZmw0pC","executionInfo":{"status":"ok","timestamp":1606162077274,"user_tz":300,"elapsed":490,"user":{"displayName":"taco rich","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp4OrQNUcok6g2RgLONXM6CjW0d0tw7tSqyd7G=s64","userId":"12470220693402591167"}}},"source":["#@save\n","def copyfile(filename, target_dir):\n","    \"\"\"Copy a file into a target directory.\"\"\"\n","    os.makedirs(target_dir, exist_ok=True)\n","    shutil.copy(filename, target_dir)\n","\n","#@save\n","def reorg_train_valid(data_dir, labels, valid_ratio):\n","    # The number of examples of the class with the least examples in the\n","    # training dataset\n","    n = collections.Counter(labels.values()).most_common()[-1][1]\n","    # The number of examples per class for the validation set\n","    n_valid_per_label = max(1, math.floor(n * valid_ratio))\n","    label_count = {}\n","    for train_file in os.listdir(os.path.join(data_dir, 'train')):\n","        label = labels[train_file.split('.')[0]]\n","        fname = os.path.join(data_dir, 'train', train_file)\n","        # Copy to train_valid_test/train_valid with a subfolder per class\n","        copyfile(fname, os.path.join(data_dir, 'train_valid_test',\n","                                     'train_valid', label))\n","        if label not in label_count or label_count[label] < n_valid_per_label:\n","            # Copy to train_valid_test/valid\n","            copyfile(fname, os.path.join(data_dir, 'train_valid_test',\n","                                         'valid', label))\n","            label_count[label] = label_count.get(label, 0) + 1\n","        else:\n","            # Copy to train_valid_test/train\n","            copyfile(fname, os.path.join(data_dir, 'train_valid_test',\n","                                         'train', label))\n","    return n_valid_per_label"],"execution_count":50,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s53N6s4rw0pC"},"source":["The `reorg_test` function below is used to organize the testing set to facilitate the reading during prediction.\n"]},{"cell_type":"code","metadata":{"id":"rCy5SpdYw0pC","executionInfo":{"status":"ok","timestamp":1606162077276,"user_tz":300,"elapsed":484,"user":{"displayName":"taco rich","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp4OrQNUcok6g2RgLONXM6CjW0d0tw7tSqyd7G=s64","userId":"12470220693402591167"}}},"source":["#@save\n","def reorg_test(data_dir):\n","    for test_file in os.listdir(os.path.join(data_dir, 'test')):\n","        copyfile(os.path.join(data_dir, 'test', test_file),\n","                 os.path.join(data_dir, 'train_valid_test', 'test',\n","                              'unknown'))"],"execution_count":51,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"09qKHPfDw0pC"},"source":["Finally, we use a function to call the previously defined `read_csv_labels`, `reorg_train_valid`, and `reorg_test` functions.\n"]},{"cell_type":"code","metadata":{"id":"FI-30016w0pC","executionInfo":{"status":"ok","timestamp":1606162077278,"user_tz":300,"elapsed":480,"user":{"displayName":"taco rich","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp4OrQNUcok6g2RgLONXM6CjW0d0tw7tSqyd7G=s64","userId":"12470220693402591167"}}},"source":["def reorg_cifar10_data(data_dir, valid_ratio):\n","    labels = read_csv_labels(os.path.join(data_dir, 'trainLabels.csv'))\n","    reorg_train_valid(data_dir, labels, valid_ratio)\n","    reorg_test(data_dir)"],"execution_count":52,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7J2z_5H-w0pC"},"source":["We only set the batch size to $4$ for the demo dataset. During actual training and testing, the complete dataset of the Kaggle competition should be used and `batch_size` should be set to a larger integer, such as $128$. We use $10\\%$ of the training examples as the validation set for tuning hyperparameters.\n"]},{"cell_type":"code","metadata":{"id":"XQxmWb-Dw0pC","colab":{"base_uri":"https://localhost:8080/","height":362},"executionInfo":{"status":"error","timestamp":1606162507517,"user_tz":300,"elapsed":351,"user":{"displayName":"taco rich","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp4OrQNUcok6g2RgLONXM6CjW0d0tw7tSqyd7G=s64","userId":"12470220693402591167"}},"outputId":"c01982df-b30a-412d-fadc-0b2fae7d6bb6"},"source":["batch_size = 4 if demo else 128\n","valid_ratio = 0.1\n","reorg_cifar10_data(data_dir, valid_ratio)"],"execution_count":55,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-55-625ee2a893d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdemo\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvalid_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mreorg_cifar10_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-52-e4f6a0b176ad>\u001b[0m in \u001b[0;36mreorg_cifar10_data\u001b[0;34m(data_dir, valid_ratio)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreorg_cifar10_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_csv_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'trainLabels.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mreorg_train_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mreorg_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-50-c5bd0faec8e6>\u001b[0m in \u001b[0;36mreorg_train_valid\u001b[0;34m(data_dir, labels, valid_ratio)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mlabel_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrain_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Copy to train_valid_test/train_valid with a subfolder per class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'train'"]}]},{"cell_type":"markdown","metadata":{"id":"5e_G6jULw0pC"},"source":["## Image Augmentation\n","\n","To cope with overfitting, we use image augmentation. For example, by adding `transforms.RandomFlipLeftRight()`, the images can be flipped at random. We can also perform normalization for the three RGB channels of color images using `transforms.Normalize()`. Below, we list some of these operations that you can choose to use or modify depending on requirements.\n"]},{"cell_type":"code","metadata":{"id":"NsShHPM0w0pD","executionInfo":{"status":"aborted","timestamp":1606162077481,"user_tz":300,"elapsed":671,"user":{"displayName":"taco rich","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp4OrQNUcok6g2RgLONXM6CjW0d0tw7tSqyd7G=s64","userId":"12470220693402591167"}}},"source":["transform_train = gluon.data.vision.transforms.Compose([\n","    # Magnify the image to a square of 40 pixels in both height and width\n","    gluon.data.vision.transforms.Resize(40),\n","    # Randomly crop a square image of 40 pixels in both height and width to\n","    # produce a small square of 0.64 to 1 times the area of the original\n","    # image, and then shrink it to a square of 32 pixels in both height and\n","    # width\n","    gluon.data.vision.transforms.RandomResizedCrop(32, scale=(0.64, 1.0),\n","                                                   ratio=(1.0, 1.0)),\n","    gluon.data.vision.transforms.RandomFlipLeftRight(),\n","    gluon.data.vision.transforms.ToTensor(),\n","    # Normalize each channel of the image\n","    gluon.data.vision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n","                                           [0.2023, 0.1994, 0.2010])])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bchEw3Acw0pD"},"source":["In order to ensure the certainty of the output during testing, we only perform normalization on the image.\n"]},{"cell_type":"code","metadata":{"id":"SPAGLbSrw0pD","executionInfo":{"status":"aborted","timestamp":1606162077482,"user_tz":300,"elapsed":667,"user":{"displayName":"taco rich","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp4OrQNUcok6g2RgLONXM6CjW0d0tw7tSqyd7G=s64","userId":"12470220693402591167"}}},"source":["transform_test = gluon.data.vision.transforms.Compose([\n","    gluon.data.vision.transforms.ToTensor(),\n","    gluon.data.vision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n","                                           [0.2023, 0.1994, 0.2010])])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LeoFaToTw0pD"},"source":["## Reading the Dataset\n","\n","Next, we can create the `ImageFolderDataset` instance to read the organized dataset containing the original image files, where each example includes the image and label.\n"]},{"cell_type":"code","metadata":{"id":"qOt4FeAOw0pD","executionInfo":{"status":"aborted","timestamp":1606162077485,"user_tz":300,"elapsed":666,"user":{"displayName":"taco rich","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp4OrQNUcok6g2RgLONXM6CjW0d0tw7tSqyd7G=s64","userId":"12470220693402591167"}}},"source":["train_ds, valid_ds, train_valid_ds, test_ds = [\n","    gluon.data.vision.ImageFolderDataset(\n","        os.path.join(data_dir, 'train_valid_test', folder))\n","    for folder in ['train', 'valid', 'train_valid', 'test']]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vucmwgX1w0pD"},"source":["We specify the defined image augmentation operation in `DataLoader`. During training, we only use the validation set to evaluate the model, so we need to ensure the certainty of the output. During prediction, we will train the model on the combined training set and validation set to make full use of all labelled data.\n"]},{"cell_type":"code","metadata":{"id":"OzUcU_W3w0pD","executionInfo":{"status":"aborted","timestamp":1606162077485,"user_tz":300,"elapsed":660,"user":{"displayName":"taco rich","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp4OrQNUcok6g2RgLONXM6CjW0d0tw7tSqyd7G=s64","userId":"12470220693402591167"}}},"source":["train_iter, train_valid_iter = [gluon.data.DataLoader(\n","    dataset.transform_first(transform_train), batch_size, shuffle=True,\n","    last_batch='discard') for dataset in (train_ds, train_valid_ds)]\n","\n","valid_iter = gluon.data.DataLoader(\n","    valid_ds.transform_first(transform_test), batch_size, shuffle=False,\n","    last_batch='discard')\n","\n","test_iter = gluon.data.DataLoader(\n","    test_ds.transform_first(transform_test), batch_size, shuffle=False,\n","    last_batch='keep')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"htOEJMIcw0pD"},"source":["## Defining the Model\n","\n","Here, we build the residual blocks based on the `HybridBlock` class, which is\n","slightly different than the implementation described in\n",":numref:`sec_resnet`. This is done to improve execution efficiency.\n"]},{"cell_type":"code","metadata":{"id":"oibk699Vw0pD","executionInfo":{"status":"aborted","timestamp":1606162077486,"user_tz":300,"elapsed":657,"user":{"displayName":"taco rich","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp4OrQNUcok6g2RgLONXM6CjW0d0tw7tSqyd7G=s64","userId":"12470220693402591167"}}},"source":["class Residual(nn.HybridBlock):\n","    def __init__(self, num_channels, use_1x1conv=False, strides=1, **kwargs):\n","        super(Residual, self).__init__(**kwargs)\n","        self.conv1 = nn.Conv2D(num_channels, kernel_size=3, padding=1,\n","                               strides=strides)\n","        self.conv2 = nn.Conv2D(num_channels, kernel_size=3, padding=1)\n","        if use_1x1conv:\n","            self.conv3 = nn.Conv2D(num_channels, kernel_size=1,\n","                                   strides=strides)\n","        else:\n","            self.conv3 = None\n","        self.bn1 = nn.BatchNorm()\n","        self.bn2 = nn.BatchNorm()\n","\n","    def hybrid_forward(self, F, X):\n","        Y = F.npx.relu(self.bn1(self.conv1(X)))\n","        Y = self.bn2(self.conv2(Y))\n","        if self.conv3:\n","            X = self.conv3(X)\n","        return F.npx.relu(Y + X)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qPAYHAHGw0pE"},"source":["Next, we define the ResNet-18 model.\n"]},{"cell_type":"code","metadata":{"id":"WTPcH0Oow0pE","executionInfo":{"status":"aborted","timestamp":1606162077487,"user_tz":300,"elapsed":652,"user":{"displayName":"taco rich","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp4OrQNUcok6g2RgLONXM6CjW0d0tw7tSqyd7G=s64","userId":"12470220693402591167"}}},"source":["def resnet18(num_classes):\n","    net = nn.HybridSequential()\n","    net.add(nn.Conv2D(64, kernel_size=3, strides=1, padding=1),\n","            nn.BatchNorm(), nn.Activation('relu'))\n","\n","    def resnet_block(num_channels, num_residuals, first_block=False):\n","        blk = nn.HybridSequential()\n","        for i in range(num_residuals):\n","            if i == 0 and not first_block:\n","                blk.add(Residual(num_channels, use_1x1conv=True, strides=2))\n","            else:\n","                blk.add(Residual(num_channels))\n","        return blk\n","\n","    net.add(resnet_block(64, 2, first_block=True),\n","            resnet_block(128, 2),\n","            resnet_block(256, 2),\n","            resnet_block(512, 2))\n","    net.add(nn.GlobalAvgPool2D(), nn.Dense(num_classes))\n","    return net"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PVVLCSjAw0pE"},"source":["The CIFAR-10 image classification challenge uses 10 categories. We will perform Xavier random initialization on the model before training begins.\n"]},{"cell_type":"code","metadata":{"id":"QFJ5Y12xw0pE","executionInfo":{"status":"aborted","timestamp":1606162077488,"user_tz":300,"elapsed":650,"user":{"displayName":"taco rich","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp4OrQNUcok6g2RgLONXM6CjW0d0tw7tSqyd7G=s64","userId":"12470220693402591167"}}},"source":["def get_net(devices):\n","    num_classes = 10\n","    net = resnet18(num_classes)\n","    net.initialize(ctx=devices, init=init.Xavier())\n","    return net\n","\n","loss = gluon.loss.SoftmaxCrossEntropyLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uRcxRVIQw0pE"},"source":["## Defining the Training Functions\n","\n","We will select the model and tune hyperparameters according to the model's performance on the validation set. Next, we define the model training function `train`. We record the training time of each epoch, which helps us compare the time costs of different models.\n"]},{"cell_type":"code","metadata":{"id":"L6UPIkjww0pE","executionInfo":{"status":"aborted","timestamp":1606162077491,"user_tz":300,"elapsed":648,"user":{"displayName":"taco rich","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp4OrQNUcok6g2RgLONXM6CjW0d0tw7tSqyd7G=s64","userId":"12470220693402591167"}}},"source":["def train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\n","          lr_decay):\n","    trainer = gluon.Trainer(net.collect_params(), 'sgd',\n","                            {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n","    num_batches, timer = len(train_iter), d2l.Timer()\n","    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n","                            legend=['train loss', 'train acc', 'valid acc'])\n","    for epoch in range(num_epochs):\n","        metric = d2l.Accumulator(3)\n","        if epoch > 0 and epoch % lr_period == 0:\n","            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n","        for i, (features, labels) in enumerate(train_iter):\n","            timer.start()\n","            l, acc = d2l.train_batch_ch13(\n","                net, features, labels.astype('float32'), loss, trainer,\n","                devices, d2l.split_batch)\n","            metric.add(l, acc, labels.shape[0])\n","            timer.stop()\n","            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n","                animator.add(epoch + (i + 1) / num_batches,\n","                             (metric[0] / metric[2], metric[1] / metric[2],\n","                              None))\n","        if valid_iter is not None:\n","            valid_acc = d2l.evaluate_accuracy_gpus(net, valid_iter,\n","                                                   d2l.split_batch)\n","            animator.add(epoch + 1, (None, None, valid_acc))\n","    if valid_iter is not None:\n","        print(f'loss {metric[0] / metric[2]:.3f}, '\n","              f'train acc {metric[1] / metric[2]:.3f}, '\n","              f'valid acc {valid_acc:.3f}')\n","    else:\n","        print(f'loss {metric[0] / metric[2]:.3f}, '\n","              f'train acc {metric[1] / metric[2]:.3f}')\n","    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\n","          f'on {str(devices)}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7fa5cJl4w0pE"},"source":["## Training and Validating the Model\n","\n","Now, we can train and validate the model. The following hyperparameters can be tuned. For example, we can increase the number of epochs. Because `lr_period` and `lr_decay` are set to 50 and 0.1 respectively, the learning rate of the optimization algorithm will be multiplied by 0.1 after every 50 epochs. For simplicity, we only train one epoch here.\n"]},{"cell_type":"code","metadata":{"id":"CzdIIt-Bw0pE","executionInfo":{"status":"aborted","timestamp":1606162077492,"user_tz":300,"elapsed":645,"user":{"displayName":"taco rich","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp4OrQNUcok6g2RgLONXM6CjW0d0tw7tSqyd7G=s64","userId":"12470220693402591167"}}},"source":["devices, num_epochs, lr, wd = d2l.try_all_gpus(), 5, 0.1, 5e-4\n","lr_period, lr_decay, net = 50, 0.1, get_net(devices)\n","net.hybridize()\n","train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\n","      lr_decay)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HfyN9Br5w0pE"},"source":["## Classifying the Testing Set and Submitting Results on Kaggle\n","\n","After obtaining a satisfactory model design and hyperparameters, we use all training datasets (including validation sets) to retrain the model and classify the testing set.\n"]},{"cell_type":"code","metadata":{"id":"Z8cnfP_rw0pE","executionInfo":{"status":"aborted","timestamp":1606162077492,"user_tz":300,"elapsed":640,"user":{"displayName":"taco rich","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp4OrQNUcok6g2RgLONXM6CjW0d0tw7tSqyd7G=s64","userId":"12470220693402591167"}}},"source":["net, preds = get_net(devices), []\n","net.hybridize()\n","train(net, train_valid_iter, None, num_epochs, lr, wd, devices, lr_period,\n","      lr_decay)\n","\n","for X, _ in test_iter:\n","    y_hat = net(X.as_in_ctx(devices[0]))\n","    preds.extend(y_hat.argmax(axis=1).astype(int).asnumpy())\n","sorted_ids = list(range(1, len(test_ds) + 1))\n","sorted_ids.sort(key=lambda x: str(x))\n","df = pd.DataFrame({'id': sorted_ids, 'label': preds})\n","df['label'] = df['label'].apply(lambda x: train_valid_ds.synsets[x])\n","df.to_csv('submission.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wb-fMciow0pE"},"source":["After executing the above code, we will get a \"submission.csv\" file. The format\n","of this file is consistent with the Kaggle competition requirements. The method\n","for submitting results is similar to method in :numref:`sec_kaggle_house`.\n","\n","## Summary\n","\n","* We can create an `ImageFolderDataset` instance to read the dataset containing the original image files.\n","* We can use convolutional neural networks, image augmentation, and hybrid programming to take part in an image classification competition.\n","\n","\n","## Exercises\n","\n","1. Use the complete CIFAR-10 dataset for the Kaggle competition. Change the `batch_size` and number of epochs `num_epochs` to 128 and 100, respectively.  See what accuracy and ranking you can achieve in this competition.\n","1. What accuracy can you achieve when not using image augmentation?\n","1. Scan the QR code to access the relevant discussions and exchange ideas about the methods used and the results obtained with the community. Can you come up with any better techniques?\n"]},{"cell_type":"markdown","metadata":{"id":"owEmjhocw0pE"},"source":["[Discussions](https://discuss.d2l.ai/t/379)\n"]}]}